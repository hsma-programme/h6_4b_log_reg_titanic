{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSMA Exercise\n",
    "\n",
    "The data loaded in this exercise is for seven acute stroke units, and whether a patient receives clost-busting treatment for stroke.  There are lots of features, and a description of the features can be found in the file stroke_data_feature_descriptions.csv.\n",
    "\n",
    "Train a Logistic Regression model to try to predict whether or not a stroke patient receives clot-busting treatment.  Use the prompts below to write each section of code.\n",
    "\n",
    "What do you conclude are the most important features for predicting whether a patient receives clot busting treatment?  Can you improve accuracy by changing the size of your train / test split?  If you have time, perhaps consider dropping some features from your data based on your outputs (in the same way you dropped passengerID in the Titanic example).  Don't forget you'll need to rerun all subsequent cells if you make changes like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Import machine learning methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Download data \n",
    "# (not required if running locally and have previously downloaded data)\n",
    "\n",
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '2004_titanic/master/jupyter_notebooks/data/hsma_stroke.csv'        \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data to data subfolder\n",
    "    data.to_csv(data_directory + 'hsma_stroke.csv', index=False)\n",
    "    \n",
    "# Load data    \n",
    "data = pd.read_csv('data/hsma_stroke.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)\n",
    "# Show data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at overview of data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at mean feature values for those who were given a clotbuster vs those\n",
    "# that weren't\n",
    "mask = data['Clotbuster given'] == 1\n",
    "given = data[mask]\n",
    "\n",
    "mask = data['Clotbuster given'] == 0\n",
    "not_given = data[mask]\n",
    "\n",
    "summary = pd.DataFrame()\n",
    "summary['given'] = given.mean()\n",
    "summary['not given'] = not_given.mean()\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into features and labels\n",
    "X = data.drop('Clotbuster given', axis=1)\n",
    "y = data['Clotbuster given']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise data\n",
    "def standardise_data(X_train, X_test):\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler() \n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.fit_transform(X_train)\n",
    "    test_std=sc.fit_transform(X_test)\n",
    "    \n",
    "    return train_std, test_std\n",
    "\n",
    "X_train_std, X_test_std = standardise_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit (train) Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict training and test labels, and calculate accuracy\n",
    "y_pred_train = model.predict(X_train_std)\n",
    "y_pred_test = model.predict(X_test_std)\n",
    "\n",
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print (f'Accuracy of predicting training data = {accuracy_train}')\n",
    "print (f'Accuracy of predicting test data = {accuracy_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine feature weights and sort by most influential\n",
    "co_eff = model.coef_[0]\n",
    "\n",
    "co_eff_df = pd.DataFrame()\n",
    "co_eff_df['feature'] = list(X)\n",
    "co_eff_df['co_eff'] = co_eff\n",
    "co_eff_df['abs_co_eff'] = np.abs(co_eff)\n",
    "co_eff_df.sort_values(by='abs_co_eff', ascending=False, inplace=True)\n",
    "\n",
    "co_eff_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
